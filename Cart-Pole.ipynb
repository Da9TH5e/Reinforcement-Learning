{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a4178d-efb7-4974-ae9c-1ff64d45498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa3729bb-a561-4661-894b-e63e18592b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b423d8bc-9e72-4a2f-8cec-a0d8ef4c9bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:15.0\n",
      "Episode:2 Score:10.0\n",
      "Episode:3 Score:30.0\n",
      "Episode:4 Score:13.0\n",
      "Episode:5 Score:41.0\n"
     ]
    }
   ],
   "source": [
    "episodes  = 5\n",
    "for episodes in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episodes, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69bbe29c-e6ad-4c8c-aa01-fded956e3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0454b19e-46c7-4970-9385-e23646527cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ccbe09-e1e3-4a1a-90c3-37d1e1804778",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_CartPole_v0')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f07d9b1-4a72-43bf-a426-e0a3f978a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_24\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 590  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008288993 |\n",
      "|    clip_fraction        | 0.0786      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.0187     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.36        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 46.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012969796 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 29.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 428          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077707833 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.636       |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062521594 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.9         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 66.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 422         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008141165 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049074795 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.581       |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.87         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 53.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008415898 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008733051 |\n",
      "|    clip_fraction        | 0.0461      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.23        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 409          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052532246 |\n",
      "|    clip_fraction        | 0.0712       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00871     |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27ea1f6f340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593575bc-336a-4096-b705-dcd3f596e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debar\\anaconda3\\envs\\rl_env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b216533-d4b0-4e46-8e01-ff3f9187d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:963.0\n",
      "Episode:2 Score:249.0\n",
      "Episode:3 Score:1102.0\n",
      "Episode:4 Score:1071.0\n",
      "Episode:5 Score:364.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "episodes  = 5\n",
    "for episodes in range(1, episodes+1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episodes, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b1cb44-cf92-4724-91f1-eee740fe7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a681d9cb-e386-41d7-9f4e-82a2c90e4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1c5079e-dfb4-47c7-aa20-92ae518bc199",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "547797c0-7e46-44a2-b575-cac666717845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b017dbb-ce0c-43ff-9046-ce24d8187345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_26\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1296 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1051        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009100579 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000295   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.89        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1013        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007634298 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0799      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 40          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 996          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070978627 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.641       |\n",
      "|    explained_variance   | 0.204        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 59           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=360.20 +/- 134.90\n",
      "Episode length: 360.20 +/- 134.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 360          |\n",
      "|    mean_reward          | 360          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056813234 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.2         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 80           |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 360.20  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27ea7d56ef0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e13df8dc-23e6-4ce1-9441-73971ae50e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16a7af4-fe7b-4eb8-92b0-9ce79ea31413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debar\\anaconda3\\envs\\rl_env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f40349e1-b0a6-477c-baf1-bdaa48d1e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_27\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1228 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014268395 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | -0.00157    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.57        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 18.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 712          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154499225 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0.481        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.06         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    value_loss           | 22.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 684         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013854409 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=205.00 +/- 78.08\n",
      "Episode length: 205.00 +/- 78.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 205          |\n",
      "|    mean_reward          | 205          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154295545 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.59         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0197      |\n",
      "|    value_loss           | 31.7         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 646   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 618       |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 19        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0111719 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.56     |\n",
      "|    explained_variance   | 0.844     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89      |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0153   |\n",
      "|    value_loss           | 19.2      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010031489 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.52        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013702952 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 615          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077139987 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.547       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 11.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=483.00 +/- 34.00\n",
      "Episode length: 483.00 +/- 34.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | 483          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066994308 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.319        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    value_loss           | 4.18         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 483.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27ea7d54940>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12909b-a660-4b0e-bc0f-f97a7e9a5305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
